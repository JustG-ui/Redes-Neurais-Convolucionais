{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JustG-ui/Redes-Neurais-Convolucionais/blob/main/2025_1_Treinamento_de_modelo_CNN_para_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adbIxuLJm_ek",
        "outputId": "e5908e30-d870-4834-e278-b9f0a7239a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cpu\n"
          ]
        }
      ],
      "source": [
        "# Nomes dos alunos: Henrique Vitral, Henrique Costa Gomes, Lucas Seabra, João Pedro Boanerges\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Configuração de dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "## 1. Download e Preparação do Dataset MNIST\n",
        "def carregar_dados():\n",
        "    \"\"\"\n",
        "    Carrega os datasets MNIST de treino e teste.\n",
        "\n",
        "    Retorna:\n",
        "        train_loader: DataLoader para dados de treino\n",
        "        test_loader: DataLoader para dados de teste\n",
        "    \"\"\"\n",
        "    # Transformações aplicadas nas imagens\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    # Download dos datasets\n",
        "    train_dataset = datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    test_dataset = datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    # Cria os DataLoaders para carregar os dados em batches\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=1000,\n",
        "        shuffle=False,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "    return train_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "## 2. Definição da Arquitetura CNN\n",
        "class CNN_MNIST(nn.Module):\n",
        "    \"\"\"\n",
        "    Rede Neural Convolucional para classificação do dataset MNIST.\n",
        "\n",
        "    Arquitetura:\n",
        "    - Conv2d (1->32) + ReLU + Conv2d (32->64) + ReLU + MaxPool + Dropout\n",
        "    - Conv2d (64->128) + ReLU + MaxPool + Dropout\n",
        "    - Flatten + Linear (128*3*3->256) + ReLU + Dropout + Linear (256->10)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CNN_MNIST, self).__init__()\n",
        "\n",
        "        # Primeira camada convolucional\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "\n",
        "        # Segunda camada convolucional\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "\n",
        "        # Camadas totalmente conectadas\n",
        "        self.fc1 = nn.Linear(128 * 7 * 7, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "        # Dropout para regularização\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Primeira sequência convolucional\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Segunda sequência convolucional\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Flatten e camadas totalmente conectadas\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "UC5xDnD3nJh4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Funções de Treino e Teste\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def treinar(modelo, dispositivo, train_loader, optimizer, epoch):\n",
        "    \"\"\"\n",
        "    Executa uma época de treinamento do modelo.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo CNN a ser treinado\n",
        "        dispositivo: Dispositivo onde os cálculos serão feitos (CPU/GPU)\n",
        "        train_loader: DataLoader com dados de treino\n",
        "        optimizer: Otimizador para ajuste dos pesos\n",
        "        epoch: Número da época atual\n",
        "\n",
        "    Retorna:\n",
        "        loss_medio: Loss médio da época\n",
        "        acuracia: Acurácia do modelo nos dados de treino\n",
        "    \"\"\"\n",
        "    modelo.train()  # Modo treino\n",
        "    loss_total = 0\n",
        "    corretos = 0\n",
        "\n",
        "    for batch_idx, (dados, alvos) in enumerate(train_loader):\n",
        "        dados, alvos = dados.to(dispositivo), alvos.to(dispositivo)\n",
        "\n",
        "        # Zera os gradientes\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = modelo(dados)\n",
        "\n",
        "        # Calcula o loss\n",
        "        loss = nn.functional.nll_loss(output, alvos)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Atualiza os pesos\n",
        "        optimizer.step()\n",
        "\n",
        "        # Acumula estatísticas\n",
        "        loss_total += loss.item()\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        corretos += pred.eq(alvos.view_as(pred)).sum().item()\n",
        "\n",
        "    # Calcula médias\n",
        "    tamanho_dataset = len(train_loader.dataset)\n",
        "    loss_medio = loss_total / len(train_loader)\n",
        "    acuracia = 100. * corretos / tamanho_dataset\n",
        "\n",
        "    print(f\"Treino - Época {epoch}: Loss médio: {loss_medio:.4f}, Acurácia: {corretos}/{tamanho_dataset} ({acuracia:.2f}%)\")\n",
        "\n",
        "    return loss_medio, acuracia\n",
        "\n",
        "def testar(modelo, dispositivo, test_loader):\n",
        "    \"\"\"\n",
        "    Avalia o modelo nos dados de teste.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo CNN a ser avaliado\n",
        "        dispositivo: Dispositivo onde os cálculos serão feitos (CPU/GPU)\n",
        "        test_loader: DataLoader com dados de teste\n",
        "\n",
        "    Retorna:\n",
        "        loss_medio: Loss médio no conjunto de teste\n",
        "        acuracia: Acurácia do modelo nos dados de teste\n",
        "    \"\"\"\n",
        "    modelo.eval()\n",
        "    loss_total = 0\n",
        "    corretos = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for dados, alvos in test_loader:\n",
        "            dados, alvos = dados.to(dispositivo), alvos.to(dispositivo)\n",
        "\n",
        "            # Forward pass\n",
        "            output = modelo(dados)\n",
        "\n",
        "            # Calcula o loss\n",
        "            loss_total += nn.functional.nll_loss(output, alvos, reduction='sum').item()\n",
        "\n",
        "            # Calcula acertos\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            corretos += pred.eq(alvos.view_as(pred)).sum().item()\n",
        "\n",
        "    # Calcula médias\n",
        "    tamanho_dataset = len(test_loader.dataset)\n",
        "    loss_medio = loss_total / tamanho_dataset\n",
        "    acuracia = 100. * corretos / tamanho_dataset\n",
        "\n",
        "    print(f\"Teste - Loss médio: {loss_medio:.4f}, Acurácia: {corretos}/{tamanho_dataset} ({acuracia:.2f}%)\")\n",
        "\n",
        "    return loss_medio, acuracia"
      ],
      "metadata": {
        "id": "y-IGGMlFnPrx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 4. Treinamento com Early Stopping\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "def treinamento_com_early_stopping(modelo, dispositivo, train_loader, test_loader, max_epocas=20):\n",
        "    \"\"\"\n",
        "    Executa o treinamento do modelo com early stopping para evitar overfitting.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo CNN a ser treinado\n",
        "        dispositivo: Dispositivo onde os cálculos serão feitos (CPU/GPU)\n",
        "        train_loader: DataLoader com dados de treino\n",
        "        test_loader: DataLoader com dados de teste\n",
        "        max_epocas: Número máximo de épocas de treino\n",
        "\n",
        "    Retorna:\n",
        "        historico: Dicionário com histórico de loss e acurácia\n",
        "    \"\"\"\n",
        "    optimizer = optim.Adam(modelo.parameters(), lr=0.0001)\n",
        "    historico = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': [],\n",
        "        'melhor_loss': float('inf'),\n",
        "        'epocas_sem_melhoria': 0,\n",
        "        'melhor_epoca': 0\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, max_epocas + 1):\n",
        "        # Executa uma época de treino\n",
        "        train_loss, train_acc = treinar(modelo, dispositivo, train_loader, optimizer, epoch)\n",
        "\n",
        "        # Avalia no conjunto de teste\n",
        "        test_loss, test_acc = testar(modelo, dispositivo, test_loader)\n",
        "\n",
        "        # Armazena no histórico\n",
        "        historico['train_loss'].append(train_loss)\n",
        "        historico['train_acc'].append(train_acc)\n",
        "        historico['test_loss'].append(test_loss)\n",
        "        historico['test_acc'].append(test_acc)\n",
        "\n",
        "        # Verifica early stopping\n",
        "        if test_loss < historico['melhor_loss']:\n",
        "            historico['melhor_loss'] = test_loss\n",
        "            historico['epocas_sem_melhoria'] = 0\n",
        "            historico['melhor_epoca'] = epoch\n",
        "            # Salva o melhor modelo\n",
        "            torch.save(modelo.state_dict(), 'melhor_modelo_mnist.pt')\n",
        "        else:\n",
        "            historico['epocas_sem_melhoria'] += 1\n",
        "            if historico[\"epocas_sem_melhoria\"] >= 3:\n",
        "                print(f\"\\nEarly stopping na época {epoch}! O erro de teste não melhorou desde a época {historico['melhor_epoca']}.\")\n",
        "                break\n",
        "\n",
        "    # Carrega o melhor modelo antes do overfitting\n",
        "    modelo.load_state_dict(torch.load('melhor_modelo_mnist.pt'))\n",
        "\n",
        "    return historico\n"
      ],
      "metadata": {
        "id": "pLzltlG5nWTc"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 5. Visualização dos Resultados\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotar_resultados(historico):\n",
        "    \"\"\"\n",
        "    Plota gráficos de loss e acurácia durante o treinamento.\n",
        "\n",
        "    Args:\n",
        "        historico: Dicionário com histórico de treinamento\n",
        "    \"\"\"\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Gráfico de Loss\n",
        "    ax1.plot(historico['train_loss'], label='Treino')\n",
        "    ax1.plot(historico['test_loss'], label='Teste')\n",
        "    ax1.set_title('Loss durante o Treinamento')\n",
        "    ax1.set_xlabel('Época')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "\n",
        "    # Gráfico de Acurácia\n",
        "    ax2.plot(historico['train_acc'], label='Treino')\n",
        "    ax2.plot(historico['test_acc'], label='Teste')\n",
        "    ax2.set_title('Acurácia durante o Treinamento')\n",
        "    ax2.set_xlabel('Época')\n",
        "    ax2.set_ylabel('Acurácia (%)')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "G_6Q1GaDJrIM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 6. Exemplo de Uso do Modelo Treinado\n",
        "def exemplo_uso(modelo, dispositivo, test_loader):\n",
        "    \"\"\"\n",
        "    Mostra um exemplo de classificação usando o modelo treinado.\n",
        "\n",
        "    Args:\n",
        "        modelo: Modelo CNN treinado\n",
        "        dispositivo: Dispositivo onde os cálculos serão feitos (CPU/GPU)\n",
        "        test_loader: DataLoader com dados de teste\n",
        "    \"\"\"\n",
        "    # Pega um batch de exemplos de teste\n",
        "    data_iter = iter(test_loader)\n",
        "    dados, alvos = next(data_iter)\n",
        "\n",
        "    # Seleciona apenas 9 exemplos para visualização\n",
        "    dados = dados[:9]\n",
        "    alvos = alvos[:9]\n",
        "\n",
        "    dados, alvos = dados.to(dispositivo), alvos.to(dispositivo)\n",
        "\n",
        "    # Faz a predição\n",
        "    modelo.eval()\n",
        "    with torch.no_grad():\n",
        "        output = modelo(dados)\n",
        "\n",
        "    # Pega as predições\n",
        "    preds = output.argmax(dim=1)\n",
        "\n",
        "    # Mostra os exemplos\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(9, 9))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        img = dados[i].cpu().numpy().squeeze()\n",
        "        ax.imshow(img, cmap='gray')\n",
        "        ax.set_title(f\"Predição: {preds[i].item()}\\nReal: {alvos[i].item()}\")\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 7. Fluxo Principal\n",
        "def main():\n",
        "    # Carrega os dados\n",
        "    train_loader, test_loader = carregar_dados()\n",
        "\n",
        "    # Cria o modelo e envia para o dispositivo (GPU/CPU)\n",
        "    modelo = CNN_MNIST().to(device)\n",
        "\n",
        "    print(\"\\nIniciando treinamento...\")\n",
        "    print(\"--------------------------------------------------\")\n",
        "\n",
        "    # Executa o treinamento\n",
        "    historico = treinamento_com_early_stopping(\n",
        "        modelo,\n",
        "        device,\n",
        "        train_loader,\n",
        "        test_loader,\n",
        "        max_epocas=20\n",
        "    )\n",
        "\n",
        "    # Plota os resultados\n",
        "    plotar_resultados(historico)\n",
        "\n",
        "    # Mostra um exemplo de uso\n",
        "    print(\"\\nExemplos de classificação:\")\n",
        "    exemplo_uso(modelo, device, test_loader)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "XZm8DZQgncBM",
        "outputId": "f9ee4063-cd54-4418-e329-b0bc9b9639a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iniciando treinamento...\n",
            "--------------------------------------------------\n",
            "Treino - Época 1: Loss médio: 0.3909, Acurácia: 52714/60000 (87.86%)\n",
            "Teste - Loss médio: 0.0827, Acurácia: 9735/10000 (97.35%)\n",
            "Treino - Época 2: Loss médio: 0.1109, Acurácia: 58001/60000 (96.67%)\n",
            "Teste - Loss médio: 0.0520, Acurácia: 9829/10000 (98.29%)\n",
            "Treino - Época 3: Loss médio: 0.0778, Acurácia: 58568/60000 (97.61%)\n",
            "Teste - Loss médio: 0.0362, Acurácia: 9882/10000 (98.82%)\n",
            "Treino - Época 4: Loss médio: 0.0624, Acurácia: 58879/60000 (98.13%)\n",
            "Teste - Loss médio: 0.0314, Acurácia: 9895/10000 (98.95%)\n",
            "Treino - Época 5: Loss médio: 0.0516, Acurácia: 59029/60000 (98.38%)\n",
            "Teste - Loss médio: 0.0266, Acurácia: 9903/10000 (99.03%)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-14-1483334457.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-14-1483334457.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Executa o treinamento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     historico = treinamento_com_early_stopping(\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mmodelo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-7-1953985261.py\u001b[0m in \u001b[0;36mtreinamento_com_early_stopping\u001b[0;34m(modelo, dispositivo, train_loader, test_loader, max_epocas)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epocas\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# Executa uma época de treino\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtreinar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdispositivo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Avalia no conjunto de teste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-2011864932.py\u001b[0m in \u001b[0;36mtreinar\u001b[0;34m(modelo, dispositivo, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdados\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Calcula o loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-12-14779980.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# 28x28x32\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m        \u001b[0;31m# 28x28x64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# 14x14x64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_jit_internal.py\u001b[0m in \u001b[0;36mfn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mif_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mif_false\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_max_pool2d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mceil_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}